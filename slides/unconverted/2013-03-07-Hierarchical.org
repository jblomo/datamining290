* Hierarchical & Density :slide:

* Methods :slide:
  + Partitioning :: Construct =k= groups, evaluate fitness, improve groups
  + Hierarchical :: Agglomerate items into groups, creating "bottom-up"
    clusters; or divide set into ever smaller groups, creating "top-down"
    clusters
  + Density :: Find groups by examining continuous density within a potential
    group
  + Grid :: Chunk space into units, cluster units instead of individual records
** Algorithms :notes:
  + Partitioning :: k-means, k-medoid
  + Hierarchical :: Build groups 1 "join" at a time, examining distance between
    two things that can be joined together, if close, combine groups. Reverse:
    divisive.
  + Density :: Many of the above methods just look for distance.  This method
    tries to find groups that might be strung out, but maintain a density.  Think
    about an asteroid belt.  It is one group, but not clustered together in a way
    you typically think.
  + Grid :: Read the book

* k-means Input :slide:animate:
  + What did we supply to k-means before we ran it?
  + =k=: number of clusters
  + Clusters disjoint*
  + Hierarchical clustering builds up clusters incrementally
** Difference :notes:
   + Hierarchical can find cluster of clusters
   + Can illustrate clusters at many levels, let human intemperate what makes
     sense without guess-and-check
   + Clusters are built 1 cluster at a time, starting with all points being
     their own cluster
   + *We'll learn about "fuzzy" clustering next time, where cluster membership
     is a probability

* Agglomerative :slide:
  + All points are separate clusters
  + Find closest clusters: Join them
  + Repeat
  [[file:img/agglomerative.png]]
** Bottom-up :notes:
   + Any questions about this?
   + What does "close" mean?

* Cluster Distance :slide:
  + Minimum :: Use the two closest points
  + Maximum :: Use the two farthest points
  + Mean :: Use the mean of the two clusters
  + Average :: Sum of the distances of all pairs, divided by number of pairs
** Meta Distance :notes:
   + These are actually distance metrics for clusters that translate down to
     distance metrics for points.
   + Still need to decide distance measures for points: Euclidean, Manhattan,
     etc. And that's just for numerical distance
   + Choose based on expected cluster topology, cross validation testing using
     human observers

* Termination :slide:
  + Define have =k= clusters
  + Distance between clusters exceeds threshold
  + Fitness function for cluster
** Details :notes:
   + If you wanted to look at all potential =k=, set =k= to 1, then look at sub
     clusters
   + Distance or fitness function (eg. density or minimum intra-cluster
     similarity score) can help define =k= automatically

* Dendrogram :slide:two_col:
  [[file:img/dendrogram1.jpg]]


  + Display of clustered groups
  + Concise visualization: groups do not need to be identified or named
  + Y axis can represent iteration
** Usefulness :notes:
   + Can move up and down clustering to make sense of individual clusters

* CHAMELEON :slide:two_col:
  + Discover large number of small clusters
  + Group together small clusters
  + Join clusters with a high interconnectedness relative to their existing
    interconnectedness
  [[file:img/chameleon.png]]
** Details :notes:
   + Mix of partition & agglomerative
   + Partition by finding groups of k-nearest neighbors: A, B in the same group
     if A is a k-nearest neighbor of B.
   + Interconnectedness measured by aggregate proximity in the group, or using a
     network model the book provides details on (10.3.4)

* Results :slide:
  [[file:img/chameleon-cluster.png]]
** Properties :notes:
   + Tends to "follow" clusters as long as interconnectedness stays high

* Density: DBSCAN :slide:two_col:
  + Find "paths" of points that are in "dense" regions
  + Paths: points within a distance =e=
  + Density: surrounded by =MinPts= within region of radius =e=
  [[file:img/density-connected.png]]
** Details :notes:
   + Can find non linear "paths" to follow as long as they stay dense

* Density Trade-offs :slide:two_col:
  [[file:img/DBSCAN.png]]
  + Finds clusters of different sizes, shapes
  + DBSCAN is sensitive to the parameters used. How big is =e=?  How many
    points is "dense"?
** Details :notes:
   + img: http://en.wikipedia.org/wiki/DBSCAN

* Algorithm Choice :slide:
  + Simple techniques often work surprisingly well
  + Choose other algorithms to tackle specific problems
  + Evaluation metrics
** Lessons :notes:
   + Just like Naive Bayes, we make assumptions about our data that turn out
     to be right enough: clusters are uniformly sized, don't wander around our
     dimensioned space
   + Topic drift: tendency for a cluster to change its properties slowly over
     time: eg. articles on politics might use different words
   + Performance: many of these algos are computationally expensive, hard to
     distribute.  Book goes into run times and where to make compromises on the
     algo
   + Figure out a fitness function for your metric.  If you used these clusters
     to take action, what would be the result?

* Elbow Method :slide:two_col:
  + Calculate intra-cluster variance
  + Compare to data set variance (F-test)
  + Find point where marginal gain of explicative power decreases
  [[file:img/elbow.JPG]]

* Labels :slide:
  + Clustering is an example of unsupervised learning
  + But after clustering, humans can label clusters, and their contents
  + Now one can use homogeneity metrics to evaluate clusters
** Homogeneity :notes:
   + Gini Index
   + Entropy
   + Precision / Recall

* *Break* :slide:

#+STYLE: <link rel="stylesheet" type="text/css" href="production/common.css" />
#+STYLE: <link rel="stylesheet" type="text/css" href="production/screen.css" media="screen" />
#+STYLE: <link rel="stylesheet" type="text/css" href="production/projection.css" media="projection" />
#+STYLE: <link rel="stylesheet" type="text/css" href="production/color-blue.css" media="projection" />
#+STYLE: <link rel="stylesheet" type="text/css" href="production/presenter.css" media="presenter" />
#+STYLE: <link href='http://fonts.googleapis.com/css?family=Lobster+Two:700|Yanone+Kaffeesatz:700|Open+Sans' rel='stylesheet' type='text/css'>

#+BEGIN_HTML
<script type="text/javascript" src="production/org-html-slideshow.js"></script>
#+END_HTML

# Local Variables:
# org-export-html-style-include-default: nil
# org-export-html-style-include-scripts: nil
# buffer-file-coding-system: utf-8-unix
# End:
