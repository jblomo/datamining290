* Linear Regression :slide:

* Types of Models :slide:animate:
  + Classifiers
  + Regressions
  + Clustering
  + Outlier
** Details :notes:
  + Classifiers :: describes and distinguishes cases. Yelp may want to find a
    category for a business based on the reviews and business description
  + Regressions :: Predict a continuous value. Eg. predict a home's selling
    price given sq footage, # of bedrooms
  + Clustering :: find "natural" groups of data *without labels*
  + Outlier :: find anomalous transactions, eg. finding fraud for credit cards

* Case Study :slide:
  + Housing prices: square footage
  [[file:img/housing-regression.gif]]
** Problem :notes:
   + We'd like to know how to price a house based on the square footage
   + Let's pretend this is the data we have
   + How would we guess that value for 2500 sq ft?

* Solution? :slide:animate:
  + Find a line that represents the data
  + =y = m*x + b=
  + A line that is not very far from the points
** Prompts :notes:
   + In English, how would you solve this?
   + How to mathematically represent the line?
   + What is a good line?

* Similarity :slide:
  + Main challenges in data mining: defining a specific metric for an intuition
  + Define distance for an individual point
  + Define how to aggregate distances together
** Challenge :notes:
   + This is big problem for engineering and math (stats) in general
   + We'll cover some concepts, but if you're ever stuck, try looking in related
     fields
   + What are some of the ways we can measure distance between points?
     Euclidian, Manhattan, Euclidian == L_2 norm
   + What is a way to aggrgate numbers? sum, sum of squares, sum of logs
   + Differences between the last two?

** Log & Square :slide:two_col:
   + Log :: Useful for de-emphasizing large raw differences
   + Square :: Useful for taking the approximate absolute value
   [[file:img/logx.gif]]

* Point Distance :slide:two_col:
  + =y= distance from line
  + Intuitively: error in estimate
  + =h(x) = m*x + b=
  + =err = h(x) - y=
  [[file:img/error.gif]]
** Error :notes:
   + We want the difference from what we estimate to be the value to what the
     value actually is

* Aggregate :slide:animate:
  + =sum=
  + What about negative error?
  + Sum of squares
  + =err = sum( (h(x) - y)**2 for x,y in dataset) / len(dataset)=
** Questions :notes:
   + Now we have info about all the errors from points, how to summarize?
   + Some points have negative error, some positive? Do they cancel each other
     out?
   + Imagine data set of two points: one solutions covers lines, other divides
     them. Which is better?
   + Use our squaring trick to make sure we don't have any negative values
   + Normalize by the number of points

* Fitness Function :slide:
  + Measures the quality or cost of the solution
  + *Key* ingredient for data mining algorithms
  + If you can measure it, you can find the best solution
** Fitness :notes:
   + Function spits out a metric. Metric can be thought of as *fitness* or
     *cost*
   + Find the maximum or minimum of that metric
   + Depending on your fitness function, this can be easy or difficult
   + img: http://onlinestatbook.com

* Understanding Error :slide:
  [[file:img/Linear_regression.svg.png]]
  Several possible solutions
** Error :notes:
   + What happens to the error as we move line around?
   + Decreases until best fit, then increases
   + What happens if we plot this error? Say, slope (x) against error (y)?

* Solution as Minimization :slide:two_col:
  + Error is a parabola
  + Several methods for finding the minimum
  + Two categories: analytical, approximations
[[file:img/parabola.png]]

* Solution Approximation :slide:
  + Some fitness functions can be difficult to solve analytically
  + Alternative: iteratively get closer to the solution
  + Stop when answer is close enough
** Analytical :notes:
   + How to find the minimum of functions in general?
   + Take derivative, find 0
   + Taking derivative can be complex or impossible (discontinuities) for some
     functions, or solving for 0 is difficult
   + Instead, well keep getting closer to the minimum using the function we
     already have

* Gradient Descent :slide:two_col:
  1. Estimate current gradient (derivative)
  1. Take a step (=a * deriv=) in the direction of the gradient
  1. Step size is small, stop. Else repeat.
  [[file:img/parabola.png]]
** Steps :notes:
   + Take gradient by looking at the local derivative, or perturbating x
   + Choose =a= as step size weight: big =a= is large step size
   + If =deriv= is large, will also make you step size large.
   + If =deriv= is large, probably means you are far away from minimum
   + Keep repeating
   + What happens if =a= is too small?
   + What happens if =a= is too big?

* General Case :slide:two_col:
  + Formulate fitness function for your problem
  + Use analytics or approximations to find min/max
  + Approximations: Newton's Method, Gradient Descent
  [[file:img/error-reduce.png]]
** Approximate visualization :notes:
   + Desired output of the error as gradient descent runs
   + maybe some local problems, as step size is too big, but slowly move down to
     a small amount of error

* Support Vector Machines :slide:

* Decision Trees :slide:two_col:
  + Great for separable attributes
  + Rules operate on independent attributes
  + Classes separable along an axis/attribute
  [[file:img/tree.png]]

** Linearly Separable :slide:
   + How to handle case where separator line is not along an axis?
  [[file:img/dataset_linsep.png]]
** Details :notes:
   + Could say if =x>2= and =y>2=, but not a great intuitive fit
   + Draw a line that takes both into account
   + =y = m*x + b=
   + img: http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html

* Possibilities :slide:
  + Many lines *could* separate these classes
  [[file:img/dataset_linsep.png]]
** Best? :notes:
   + Which is the best?
   + Why?

** Best Separator :slide:two_col:
   + Best line gives the most distance between the two classes
   + Measure distance between closest points
   + Closest points == support vectors
   [[file:img/separable.jpg]]
** Points, Vectors :notes:
   + Points can be represented as vectors
   + Vector math can be easier to express succinctly
   + img: http://www.sciencedirect.com/science/article/pii/S1072751511001918

* Dimensions :slide:
  + When separating two dimensions, we need a line
  + When separating 3 dimensions?
  + 4 dimensions?
** Vocabulary :notes:
   + Plane
   + Hyperplane

* Expressing the Hyperplane :slide:animate:
  + =y = m*x + b=
  + =x_2 = m*x_1 + b=
  + =0 = m*x_1 + b - x_2=
  + =0 = [m -1] * [x_1, x_2] + b=
  + =0 = w * x + b=
** Questions :notes:
   + How do you mathematically represent a line?
   + Now, we're not going to think of a new letter for every dimension, we're
     just going to say x_1 , x_2 , x_3 ...
   + Rewrite mathematically
   + How to add more dimensions? x_22? Express x as a vector of all attributes
   + Again, don't want to come up with a bunch more letters after =m=, so use
     =w= as the matrix representing all the =m= slopes

* Challenge :slide:two_col:
  + Find =w=, =b= such that =w * x + b= maximizes the distance between the
    support vectors
  [[file:img/svm.png]]

* Maximizing Fitness Function :slide:two_col:
  + Now we have a fitness function and parameters we're trying to optimize
  + Sound familiar?
  [[file:img/svm.png]]

* Kernel Tricks :slide:two_col:
  + SVM good for linearly separable data
  + How to handle other data?
  [[file:img/svm-circular.jpg]]

** Polynomial Kernel :slide:
   + Transform it into linearly separable
   + What function can we apply to these data points to make them separable?
  [[file:img/svm-circular.jpg]]
*** Square :notes:
   + Square all of them

** Polynomial Kernel :slide:
   [[file:img/kernel-trick.jpg]]

   Now apply SVM
** Details :notes:
   + img: http://www.sciencedirect.com/science/article/pii/S1072751511001918

* *Break* :slide:

#+STYLE: <link rel="stylesheet" type="text/css" href="production/common.css" />
#+STYLE: <link rel="stylesheet" type="text/css" href="production/screen.css" media="screen" />
#+STYLE: <link rel="stylesheet" type="text/css" href="production/projection.css" media="projection" />
#+STYLE: <link rel="stylesheet" type="text/css" href="production/color-blue.css" media="projection" />
#+STYLE: <link rel="stylesheet" type="text/css" href="production/presenter.css" media="presenter" />
#+STYLE: <link href='http://fonts.googleapis.com/css?family=Lobster+Two:700|Yanone+Kaffeesatz:700|Open+Sans' rel='stylesheet' type='text/css'>

#+BEGIN_HTML
<script type="text/javascript" src="production/org-html-slideshow.js"></script>
#+END_HTML

# Local Variables:
# org-export-html-style-include-default: nil
# org-export-html-style-include-scripts: nil
# buffer-file-coding-system: utf-8-unix
# End:
