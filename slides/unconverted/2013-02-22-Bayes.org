* Classification: Bayes :slide:

* Confusion Matrix :slide:
  + What are the ways that classification can be wrong?
  |                  | Predict: Positive | Predict: Negative | 
  | Actual: Positive | True Positive     | False Negative    | 
  | Actual: Negative | False Negative    | True Negative     | 
** Obtain Data :notes:
   + How do we obtain this data?

* Testing Data :slide:two_col:
  + Data used to test a learned model
  + Test data was not used to learn
  + Where does test data come from?
  [[file:img/stork.jpg]]
** Not from storks :notes:
   + img: http://adamsparkadventures.blogspot.com/2011/09/stork-watch.html

* Training Data :slide:
  + Set aside a portion of training data to test with
  + Test data:
  [[file:img/k-fold1.png]]

** Set Aside Testing :slide:
   [[file:img/k-fold2.png]]

   Testing Data  |  Training Data
** Colors :notes:
   + Red: Testing
   + Green: Training

** Cross Validation :slide:
   [[file:img/k-fold3.png]]

   Train and test model with different subsets of data
** Testing the model :notes:
   + This is used to test the *model*
   + How well does it perform with a variety of inputs?
   + Is it robust against outliers

** K-Fold Validation :slide:
   [[file:img/k-fold4.png]]

   Test against K sections of the data
** Statistical Significance :notes:
   + Similar to the concept in stats: the more distinct samples you have, the
     better you know your data

** K-Fold Validation :slide:
   [[file:img/k-fold5.png]]

* Bayes Theorem :slide:
  [[file:img/bayes.png]]

  Can calculate a posterior given priors
** Read :notes:
   + Probability of A given B equals probability of B given A times prob of A
     divided prob of B
   + Importance is that we can figure out what future probabilities are based on
     what we've already seen

* Spam :slide:
  [[file:img/bayes-spam.png]]

  Find the probability of spam given it contains a particular word
** Words :notes:
   + What words would you associate with spam?
   + Are these the same across all people?
   + Why might you want to train a classifier per person?

* Multiple Words :slide:animate:
  + How to calculate probabilities of multiple independent events occurring?
  + Model words as independent events
  + Multiply probabilities
** Naive :notes:
   + Words are not independent
   + San? Francisco is more likely
   + But works suprisingly well in practice

* Practical concerns :slide:animate:
  + What is the probability of a word we've never seen before?
  + Underflow: multiplying numbers still everything is rounded to 0
  + Normalizing words: v1agra
** Solutions :notes:
   + divide by 0. Instead, add 1 to all words
   + using log of probabilities
   + Rules

* Ensemble :slide:
  + Using multiple models simultaneously
  + Run all classifiers over new data, take majority vote
  + Netflix Prize won with combination of models from several teams
** Requirements :notes:
   + Nice thing is that the diversity of models is important, and not so much
     the accuracy of any single model

* Bootstrap Aggregating :slide:two_col:
  + Bagging: training data collected with replacement
  + Learn models on different samples
  + Run models on new incoming data
  [[file:img/bagging.png]]
** Trade-offs :notes:
   + Fairly simple:
   + Majority vote
   + Train models independently
   + img: http://cse-wiki.unl.edu/wiki/index.php/Bagging_and_Boosting

* Boosting :slide:
  + Train classifier to catch what the last one missed
  + Train and test first classifier
  + Find classification failures
  + Weight more heavily those failures in training a new model
  + Weight models by their accuracy
** Trade-offs :notes:
   + Boosting can be susceptible to outliers
   + Longer to train
   + Observed to be more accurate

* Many Decision Trees :slide:
  + Train trees with random selection of attributes, subset of data
  + Combine trees using majority or weights
  + What to call many arbitrarily picked trees?

** Random Forests :slide:two_col:
[[file:img/green-forrest.jpg]]
   + Used successfully in many recent competitions
   + Carry over robustness properties from individual decision trees
   + Can be trained in parallel
** Parallel :notes:
   + Potentially good fit for MapReduce paradigms

#+STYLE: <link rel="stylesheet" type="text/css" href="production/common.css" />
#+STYLE: <link rel="stylesheet" type="text/css" href="production/screen.css" media="screen" />
#+STYLE: <link rel="stylesheet" type="text/css" href="production/projection.css" media="projection" />
#+STYLE: <link rel="stylesheet" type="text/css" href="production/color-blue.css" media="projection" />
#+STYLE: <link rel="stylesheet" type="text/css" href="production/presenter.css" media="presenter" />
#+STYLE: <link href='http://fonts.googleapis.com/css?family=Lobster+Two:700|Yanone+Kaffeesatz:700|Open+Sans' rel='stylesheet' type='text/css'>

#+BEGIN_HTML
<script type="text/javascript" src="production/org-html-slideshow.js"></script>
#+END_HTML

# Local Variables:
# org-export-html-style-include-default: nil
# org-export-html-style-include-scripts: nil
# buffer-file-coding-system: utf-8-unix
# End:
