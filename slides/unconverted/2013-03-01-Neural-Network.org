* Bias vs. Variance :slide:

* Trade-offs :slide:
  + Similar to precision, we make trade-offs when training models
  + Bias: How far off are the model predictions on average?
  + Variance: If we retrained with different data, how different would our
    guesses be?
** Details :notes:
   + Bias: difference in "Expected" value from models from the real value
   + Variance: difference in "Expected" value from each other
   + Variance: Another way to think about it: how specific is our model to our
     data? If we were training a tree with k-fold validation, would we get
     completely different rule sets for each set of data?
   + "Expected": These are *model type* properties. Train the model multiple
     times with different data, then evaluate all models performance

* Regression :slide:two_col:
  + Can we do better than linear regression on some data sets?
  + Polynomial regression
  + How many polynomials?
  [[file:img/overfit1.png]]
** Polynomial :notes:
   + Sure! Use a polynomial instead: =x^2= =2x - x^2 + 4x^3=
   + If you're not sure what the underlying data model is, have to test
   + img: http://cheshmi.tumblr.com/

** One :slide:
  [[file:img/overfit1.png]]
*** So-So :notes:
    + How is the bias? Not great, fair amount of error
    + How is the variance? Pretty good, assuming random sample

** Two :slide:
   [[file:img/overfit2.png]]
*** Better :notes:
   + Bias? Better, less error
   + Variance? more risky depending on which samples you get, since model
     diverges quickly

** Three :slide:
  [[file:img/overfit3.png]]
*** Worrying :notes:
   + Now getting a little weird. We're not finding the general pattern, more
     like exactly fitting a line over these points
   + If we made model with different data, we're going to get a different line

** Many :slide:
   [[file:img/overfit5.png]]
*** Now kind of ridiculous :notes:
    + Intuitively we know this is not a description of the data
    + If a point was found near the border, completely dependant on the data the
      model trained on

* Over-fitting :slide:
  + Over-fitting: reflecting the exact data given instead of the general pattern
  + High variance is a sign of over-fitting: model guesses vary with the exact
    data given
  + Avoidance: ensembles average out variance, regularization adds a cost to
    model complexity
** Avoidance :notes:
   + Ensembles combine multiple models together. Those multiple models may have
     a lot of variance, but as long as they have good Bias, we'll center in on
     the correct result
   + Remember our cost function? We wanted to minimize the error. If you add in
     a way to measure model complexity, you can add that to the cost, so that
     you are explicitly trading-off the complexity of your model with the
     quality of the solution
   + If we wanted to add a complexity cost to the previous model, what would the
     cost be dependent on?

* Neural Networks :slide:
  [[file:img/neuron_culture.jpg]]
** notes :notes:
   + img: http://adrianbowyer.blogspot.com/2010/12/hardwired.html

* Brains :slide:
  + Neural networks try to model our brains
  + Neurons/perceptrons sense input, transform it, send output
  + Neurons/perceptrons are connected together
  + Connections have different strengths

* Training :slide:
  + Learn by adjusting the strengths of the connections
  + Mathematically, strength is a weight multiplier of the output
  + When we've found the right weights

* Nomenclature :slide:two_col:
  + Input layer :: neurons whose input is determined by features
  + Hidden layer :: neurons that calculate a combination of features
  + Output layer :: neurons that express the classification
  + Weights :: numeric parameter to adjust input/output
  [[file:img/nn.png]]

* Handwriting :slide:
  + Recognize handwritten digits
  [[file:img/neuron11.gif]]
** Inputs => Outputs :notes:
   + Break up drawing cell into pixels
   + Input takes pixel=on|off
   + Output is highest valued output node, 1 for each digit
   + img: http://vv.carleton.ca/~neil/neural/neuron-d.html

* Forward Propagation :slide:
  1. Sum of inputs * weights
  1. Apply sigmoid
  1. Send output to next layer
  1. Repeat

** Repeat :slide:
   + Multiple hidden layers used to model complex feature interaction
   [[file:img/2-layer-nn.gif]]

** Sigmoid :slide:two_col:
   + Normalize input to [0,1]
   + Makes weak input weaker, strong input stronger
   + =1 / (1 + e^-input)=
   [[file:img/sigmoid.png]]]

* Example :slide:
  [[file:img/nn-fp1.png]]
** Simple :notes:
   + Simple NN with just one output
   + Output can model true/false
   + Inputs are numerical

** Weights :slide:
   [[file:img/ann2.png]]
*** Later :notes:
   + We'll discuss how weights are determined later
   + Fill in the Hidden layer with sum of inputs * weights

** Sigmoid :slide:
   [[file:img/ann3.png]]
*** Apply :notes:
   + Apply the sigmoid to the incoming signals

** Sigmoid :slide:
   [[file:img/ann4.png]]
*** Apply :notes:
   + Apply the sigmoid to the incoming signals

** Sigmoid :slide:
   [[file:img/ann5.png]]
*** Apply :notes:
   + Apply the sigmoid to the incoming signals

** Sigmoid :slide:
   [[file:img/ann6.png]]
*** Apply :notes:
   + Apply the sigmoid to the incoming signals

** Weights :slide:
   [[file:img/ann7.png]]
*** Repeat :notes:
    + Take the outputs, apply weights, sum

** Sigmoid :slide:
   [[file:img/ann8.png]]
*** Apply :notes:
   + Apply the sigmoid to the incoming signals
   + Our result is greater than 0.5, so we can assume true
   + If we had multiple outputs, we could choose the highest one

* Forward Propagation :slide:
  1. Sum of inputs * weights
  1. Apply sigmoid
  1. Send output to next layer
  1. Repeat
** Get an answer :notes:
   + Now we have *an* output, but how do we train to get the *right* output?

* Fitness Function :slide:
  + Create a fitness function that measures the error
  + Take derivative and a step in the right direction
  + Try again
** Neural Network :notes:
   + NN training is conceptually similar to gradient descent
   + We want to get closer to the answer, so we adjust our weights based on the
     amount of incorrectness in the system
   + Adjust weights, try again

* Back Propagation :slide:
  + Run forward :: O_j is output of node =j=
  + Calculate error of output layer :: Err_j = O_j(1-O_j)(T_j-O_j)
  + Caclulate error of hidden layer :: Err_j = O_j(1-O_j)*sum(Err_k*w_jk)
  + Find new weights :: w_ij = w_ij + l*Err_j*O_i
  + Repeat :: To move closer to correct weights
** Derivative :notes:
   + Derivative of the sigmoid is =O_j(1-O_j)=, so we're taking the gradient
   + =l= is the learning rate, similar to =a= step size in gradient descent

* Example :slide:
  [[file:img/ann8.png]]
** Expected :notes:
#+begin_src python
# Expected Output is 0
t_6 = 0
# Actual Output
o_6 = 0.8387
# Output Error -0.11346127339699999
err_6 = o_6*(1-o_6)*(t_6-o_6)
# Setup hidden node 5
o_5 = 0.9933 ; w_56 = 1.5
# Error for node 5 = -0.0011326458827956695
err_5 = o_5*(1-o_5)*(err_6*w_56)
# Adjust weight to 0.37298917134759924
l = 10  # learning rate
w_56 = w_56 + l*err_6*o_5
#+end_src


* Terminate Learning :slide:
  + Changes in weights too small
  + Accuracy in training models is high
  + Maximum number or times for learning
** Forward and Back :notes:
   + Guess, correct, guess, correct
   + Stop when you've got a good model
   + or you model is not improving
   + or when you're out of time

* *Break* :slide:

#+STYLE: <link rel="stylesheet" type="text/css" href="production/common.css" />
#+STYLE: <link rel="stylesheet" type="text/css" href="production/screen.css" media="screen" />
#+STYLE: <link rel="stylesheet" type="text/css" href="production/projection.css" media="projection" />
#+STYLE: <link rel="stylesheet" type="text/css" href="production/color-blue.css" media="projection" />
#+STYLE: <link rel="stylesheet" type="text/css" href="production/presenter.css" media="presenter" />
#+STYLE: <link href='http://fonts.googleapis.com/css?family=Lobster+Two:700|Yanone+Kaffeesatz:700|Open+Sans' rel='stylesheet' type='text/css'>

#+BEGIN_HTML
<script type="text/javascript" src="production/org-html-slideshow.js"></script>
#+END_HTML

# Local Variables:
# org-export-html-style-include-default: nil
# org-export-html-style-include-scripts: nil
# buffer-file-coding-system: utf-8-unix
# End:
